# -*- coding: utf-8 -*-
"""Investo.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jQk4t4IGyl_7s2qHQROLOGlaiMERwOU2
"""

import yfinance as yf
import pandas as pd
import numpy as np

# Define the tickers for the 5-10 stocks
tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA']

# Fetch the data for each ticker
stock_data = {ticker: yf.download(ticker, start='2023-01-01', end='2023-12-31') for ticker in tickers}

# Clean the data
for ticker, data in stock_data.items():
    # Forward fill missing values
    data.fillna(method='ffill', inplace=True)
    # Drop rows with any remaining missing values
    data.dropna(inplace=True)
    # Ensure data is indexed by date
    data.index = pd.to_datetime(data.index)
    stock_data[ticker] = data
    df = pd.read_csv("/content/all_stocks_5yr.csv")

df.head()

import matplotlib.pyplot as plt
import seaborn as sns

# Plot historical prices and volumes
for ticker, data in stock_data.items():
    plt.figure(figsize=(14, 7))

    # Plot closing price
    plt.subplot(2, 1, 1)
    data['Close'].plot(title=f'{ticker} Closing Price')
    plt.xlabel('')

    # Plot volume
    plt.subplot(2, 1, 2)
    data['Volume'].plot(title=f'{ticker} Volume')
    plt.xlabel('Date')

    plt.tight_layout()
    plt.show()

# Create lagged features, rolling means, and percentage changes
for ticker, data in stock_data.items():
    data['Lag1'] = data['Close'].shift(1)
    data['Lag2'] = data['Close'].shift(2)
    data['RollingMean5'] = data['Close'].rolling(window=5).mean()
    data['RollingMean10'] = data['Close'].rolling(window=10).mean()
    data['PctChange'] = data['Close'].pct_change()
    stock_data[ticker] = data.dropna()

from statsmodels.tsa.arima.model import ARIMA
import statsmodels.api as sm

# Example for one stock (e.g., AAPL)
data = stock_data['AAPL']['Close']

# Determine optimal p, d, q
fig, ax = plt.subplots(2, 1, figsize=(14, 8))
fig = sm.graphics.tsa.plot_acf(data, lags=20, ax=ax[0])
fig = sm.graphics.tsa.plot_pacf(data, lags=20, ax=ax[1])
plt.show()

# Fit ARIMA model
model = ARIMA(data, order=(5, 1, 0))
model_fit = model.fit()
print(model_fit.summary())

# Forecast
forecast = model_fit.forecast(steps=30)
plt.figure(figsize=(10, 6))
plt.plot(data, label='Historical')
plt.plot(forecast, label='Forecast')
plt.legend()
plt.show()

from sklearn.ensemble import GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error

# Prepare data for Gradient Boosting
features = ['Lag1', 'Lag2', 'RollingMean5', 'RollingMean10', 'PctChange']
target = 'Close'

X = stock_data['AAPL'][features]
y = stock_data['AAPL'][target]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

# Train model
gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3)
gbr.fit(X_train, y_train)

# Predict
y_pred = gbr.predict(X_test)

# Evaluate
rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)

print(f'RMSE: {rmse}, MAE: {mae}')

plt.figure(figsize=(10, 6))
plt.plot(y_test.values, label='Actual')
plt.plot(y_pred, label='Predicted')
plt.legend()
plt.show()

from sklearn.metrics import mean_absolute_percentage_error

# ARIMA evaluation
arima_forecast = model_fit.forecast(steps=len(X_test))
arima_rmse = mean_squared_error(y_test, arima_forecast, squared=False)
arima_mae = mean_absolute_error(y_test, arima_forecast)
arima_mape = mean_absolute_percentage_error(y_test, arima_forecast)

print(f'ARIMA - RMSE: {arima_rmse}, MAE: {arima_mae}, MAPE: {arima_mape}')

# Gradient Boosting evaluation
gbr_mape = mean_absolute_percentage_error(y_test, y_pred)

print(f'Gradient Boosting - RMSE: {rmse}, MAE: {mae}, MAPE: {gbr_mape}')

# Compare performance
comparison = pd.DataFrame({
    'Model': ['ARIMA', 'Gradient Boosting'],
    'RMSE': [arima_rmse, rmse],
    'MAE': [arima_mae, mae],
    'MAPE': [arima_mape, gbr_mape]
})

print(comparison)

import yfinance as yf
import matplotlib.pyplot as plt

# Define the tickers for the stocks
tickers = ['AAPL', 'MSFT', 'GOOG', 'AMZN', 'TSLA']

# Fetch the data for each ticker
stock_data = {ticker: yf.download(ticker, start='2023-01-01', end='2023-12-31') for ticker in tickers}

# Plotting the closing prices
plt.figure(figsize=(14, 7))

for ticker, data in stock_data.items():
    # Plotting closing prices
    plt.plot(data.index, data['Close'], label=ticker)

plt.title('Daily Closing Prices of Stocks (2023)')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

import yfinance as yf
import matplotlib.pyplot as plt
import pandas as pd
from statsmodels.tsa.seasonal import seasonal_decompose

# Fetch the data for a single ticker (e.g., AAPL)
ticker = 'AAPL'
data = yf.download(ticker, start='2020-01-01', end='2023-12-31')['Close']

# Perform decomposition (additive model)
decomposition = seasonal_decompose(data, model='additive', period=252)  # Assuming yearly seasonality (252 trading days)

# Plot the decomposition components
plt.figure(figsize=(14, 10))

plt.subplot(411)
plt.plot(data, label='Original')
plt.legend()

plt.subplot(412)
plt.plot(decomposition.trend, label='Trend')
plt.legend()

plt.subplot(413)
plt.plot(decomposition.seasonal, label='Seasonal')
plt.legend()

plt.subplot(414)
plt.plot(decomposition.resid, label='Residual')
plt.legend()

plt.tight_layout()
plt.show()